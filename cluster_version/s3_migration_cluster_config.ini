; configure file for s3_migration_cluster master and worker nodes

[Basic]

JobType = PUT
# JobType = PUT | GET, PUT表示EC2跟目标S3不在一个Account，GET表示EC2跟源S3不在一个Account

table_queue_name = s3_migrate_file_list
# table_queue_name 需与CloudFormation/CDK创建的ddb/sqs名称一致
ssm_parameter_bucket = s3_migrate_bucket_para
# ssm_parameter_bucket 需与CloudFormation/CDK创建的 parameter store 的 Name 一致
ssm_parameter_credentials = s3_migrate_credentials
;需要在ssm parameter store手工新建一个名为 "s3_migrate_credentials" 的 parameter（CloudFormation/CDK 没设这个）
;这个是跟EC2不在一个Account体系下的另一个Account的access_key
;例如EC2在Global，则这个是China Account access_key，反之EC2在中国，这就是Global Account
;EC2本Account的权限会从 IAM Role 获取。以下是这个参数的例子：
;{
;    "aws_access_key_id": "your_aws_access_key_id",
;    "aws_secret_access_key": "your_aws_secret_access_key",
;    "region": "cn-northwest-1"
;}

# 如果job是由S3触发的，所以需要在配置文件中自行定义目标S3，如果job是jobsender程序发送的，则不需要配置这些参数，会从parameter store去取
Des_bucket_default = hawkey999
Des_prefix_default = s3-migration-test/


[Mode]
StorageClass = STANDARD
# STANDARD|REDUCED_REDUNDANCY|STANDARD_IA|ONEZONE_IA|INTELLIGENT_TIERING|GLACIER|DEEP_ARCHIVE

ifVerifyMD5Twice = False
# 是否做两次的MD5校验
# ifVerifyMD5Twice 为True则一个文件完成上传合并分片之后再次进行整个文件的ETag校验MD5。
# 对于S3_TO_S3，该开关True会在断点续传的时候，也重新"下载"源文件中所有已传过的分片来计算MD5，但不会重新上传。
# 所以启用该模式下，推荐 worker node 与源S3在同Region，否则延迟会较大
# 该开关不影响每个分片上传时候的校验，即使为False也会校验每个分片MD5，但完成文件合并后则不再校验。

# Danger!!! Don't change ChunkSize unless you well understand how to clean uploaded parts
ChunkSize = 5
# 单位：MBytes，文件分片大小，不小于5M，如果发现对于某个文件分片会多于10000个，本程序会自动针对这个文件先扩大ChunkSize_auto再分片
# 不建议修改该参数。如修改，请务必手工清除所有已上传而又未完成文件合并的 Parts，否则：
#     会导致文件错误。对于启用了ifVerifyMD5Twice为True的，会校验整个文件从而发现并重传。而没有打开的话则会导致最终文件是错误的。

ResumableThreshold = 10
# 单位MBytes，小于该值的文件，则开始传文件时不查S3上已有的Multipart Upload，不做断点续传，而直接覆盖，节省性能

MaxRetry = 10
# 单个Part上传失败后，最大重试次数, type = int
MaxThread = 1
# 单文件同时working的Thread进程数量, type = int
MaxParallelFile = 1
# 并行操作文件数量, type = int
JobTimeout = 3500
# Seconds 秒, sqs队列inVisibletime 设置为 3600，中间留了100秒间隔以便前一Job刚好在complete的时候，一个重复Job被启动了

[Debug]
CleanUnfinishedUpload = False
# 建议设置 False。遇到存在现有的未完成 multipart upload id 时，可以找出来进行续传。
# True 启动传输文件时，自动清理掉 S3 上所有的未完成 multipart upload id 。也就是不做断点续传了。
# 如果调整过ChunkSize 可以用这个功能把上传过的清理掉。

LoggingLevel = INFO
# 日志输出级别 'WARNING' | 'INFO' | 'DEBUG'
# 日常建议设置为 WARNING

LocalProfileMode = False
# 一般设置为False
# True: get credential from local profile; False: get from EC2 IAM Role
# If debug on local, os type as Darwin, you don't have to set True, I will recognize it
